{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(313864, 132)\n"
     ]
    }
   ],
   "source": [
    "train, test = pd.read_csv('data/train.csv'), pd.read_csv('data/test.csv')\n",
    "cat_cols = [f for f in train.columns if 'cat' in f]\n",
    "cont_cols = [f for f in test.columns if 'cont' in f]\n",
    "\n",
    "#Merge the dataframe so that things work out for the categorical variables\n",
    "#We will separate it later \n",
    "test['loss'] = -1\n",
    "merged = pd.concat([train, test])\n",
    "print merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This is the function that does the cleaning\n",
    "def extract_mat(df, cat, cont, factorize=False):\n",
    "    arrs = []\n",
    "    for col in cont:\n",
    "        arrs.append(np.array(df[col]))    \n",
    "    for col in cat:\n",
    "        if factorize: arrs.append(np.array(pd.factorize(df[col], sort=True)[0])\n",
    "        else: arrs.append(pd.get_dummies(df[col]).as_matrix().T)\n",
    "    return np.vstack(arrs).T\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ONLY USE THIS DATASET for trees-based models, like random forest\n",
    "#Pick the columns we want to use in the dataset\n",
    "cats = cat_cols\n",
    "conts = cont_cols\n",
    "#Turn the dataframe into a numpy array\n",
    "mat = extract_mat(merged, cats, conts, factorize=True)\n",
    "#extract the training and test datasets from the big dataset\n",
    "ntrain = len(train)\n",
    "ntest = len(test)\n",
    "ntest, ntrain\n",
    "trainf, testf = mat[:ntrain], mat[ntrain:]\n",
    "trainl = np.array(train['loss'])\n",
    "test_ids = np.array(test['id'])\n",
    "feature_names = conts+cats\n",
    "utils.save_dataset('data/factorized1.npz', train_features=trainf, test_features=testf, train_labels=trainl, ids=test_ids, feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We want to make a dataset that doesn't have lots of big categorical variables\n",
    "#So we drop the ones that are very bad\n",
    "cats_to_drop = ['cat110', 'cat111', 'cat116'] #categorical variables that have a ridiculous number of possibilities\n",
    "cats = [var for var in cat_cols if var not in cats_to_drop]\n",
    "#Turn the dataframe into a numpy array\n",
    "mat = extract_mat(merged, cats, conts, factorize=False)\n",
    "#extract the training and test datasets from the big dataset\n",
    "ntrain = len(train)\n",
    "ntest = len(test)\n",
    "ntest, ntrain\n",
    "trainf, testf = mat[:ntrain], mat[ntrain:]\n",
    "trainl = np.array(train['loss'])\n",
    "test_ids = np.array(test['id'])\n",
    "feature_names = conts+cats\n",
    "utils.save_dataset('data/no_big_cats1.npz', train_features=trainf, test_features=testf, train_labels=trainl, ids=test_ids, feature_names=feature_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
